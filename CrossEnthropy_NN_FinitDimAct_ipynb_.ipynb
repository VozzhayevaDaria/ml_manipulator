{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "82CWlIFVQ7fS"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wN8o5ebZa9ob"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Агент на основе кросс-энтропии и нейронной сети"
      ],
      "metadata": {
        "id": "IEuXndffQsOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossEnthropyNNAgent(nn.Module):\n",
        "    def __init__(self, state_dim, action_n, action_max, grad_step=0.01):\n",
        "        super().__init__()\n",
        "        # количество нейронов в слоях\n",
        "        hidden1 = 200\n",
        "        hidden2 = 100\n",
        "        hidden3 = 200\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden1, hidden2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden2, hidden3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden3, action_n)\n",
        "        ) # Задаем структуру нейронной сети\n",
        "        self.softmax = nn.Softmax() # при конечномерном пространстве действий не используется\n",
        "        self.loss = Loss_func # функция ошибки для алгоритма кросс-энтропии\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=grad_step) # стохастический градиентный спуск\n",
        "        self.action_max = action_max # амплитуда действия (максималное его занчение)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.network(input) # результат работы сети на входных данных input\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = torch.FloatTensor(state)\n",
        "        logits = self.network(state) # получаем результат работы сети на векторе состояний\n",
        "        action = torch.atan(logits).detach().numpy() * self.action_max # выбираем действие на основе распределения вероятностей действий\n",
        "        return action\n",
        "\n",
        "    def update_policy(self, elite_sessions):\n",
        "        elite_states, elite_actions = [], []\n",
        "        for session in elite_sessions:\n",
        "            elite_states.extend(session['states'])\n",
        "            elite_actions.extend(session['actions'])\n",
        "\n",
        "        elite_states = torch.FloatTensor(elite_states)\n",
        "        elite_actions = torch.FloatTensor(elite_actions)\n",
        "\n",
        "        loss = self.loss(self.network(elite_states), elite_actions) # считаем функцию потерь между результатом работы сети на заданных состояниях и элитных действий\n",
        "        loss.backward() # вычисляем градиент\n",
        "        self.optimizer.step() # шаг в сторону градиентного спуска\n",
        "        self.optimizer.zero_grad() # обнуляем градиенты\n",
        "        return None\n",
        "\n",
        "def Loss_func(calculated_actions, elite_actions):\n",
        "    res = 0\n",
        "    for i in range(len(elite_actions)):\n",
        "        res += (calculated_actions[i] - elite_actions[i]).norm() ** 2\n",
        "    return res / len(elite_actions)\n"
      ],
      "metadata": {
        "id": "2O4rdq14er2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_session(env, agent, session_len, visual=False):\n",
        "    session = {}\n",
        "    states, actions = [], []\n",
        "    total_reward = 0\n",
        "\n",
        "    state = env.reset() # обнуляем окружение\n",
        "    for _ in range(session_len):\n",
        "        states.append(state)\n",
        "        action = agent.get_action(state) # по состоянию получаем конкретное действие\n",
        "        actions.append(action)\n",
        "\n",
        "        #if visual:\n",
        "        #    env.render() # визуализация (работает только из консоли)\n",
        "        state, reward, done= env.step(action) # по действие агента получаем состояние среды\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    session['states'] = states\n",
        "    session['actions'] = actions\n",
        "    session['total_reward'] = total_reward\n",
        "    return session\n",
        "\n",
        "def get_elite_sessions(sessions, q_param): # при помощи квантили выбераем лучшие траектории\n",
        "    total_rewards = np.array([session['total_reward'] for session in sessions])\n",
        "    quantile = np.quantile(total_rewards, q_param)\n",
        "\n",
        "    elite_sessions = []\n",
        "    for session in sessions:\n",
        "        if session['total_reward'] > quantile:\n",
        "            elite_sessions.append(session)\n",
        "\n",
        "    return elite_sessions"
      ],
      "metadata": {
        "id": "dfDGKElHiy-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Симулятор манипулятора"
      ],
      "metadata": {
        "id": "82CWlIFVQ7fS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Robot:\n",
        "    def __init__(self, end_1, end_2, end_3):\n",
        "        self.end_1 = end_1\n",
        "        self.end_2 = end_2\n",
        "        self.end_3 = end_3\n",
        "\n",
        "        self.slope_1, self.slope_2, self.slope_3 = 0, 0, 0\n",
        "        self.rot_1, self.rot_2, self.rot_3 = 0, 0, 0\n",
        "\n",
        "    def draw_line(self, a, b, ax, col, style): # вспомогательная функция, рисует отрезок\n",
        "        ax.plot([a[0], b[0]], [a[1], b[1]], [a[2], b[2]], color=col, linestyle=style)\n",
        "        return None\n",
        "\n",
        "    def show(self, ax, col='b', style='solid'): # рисует робота\n",
        "        self.draw_line(self.end_1, [0, 0, 0], ax, col, style)\n",
        "        self.draw_line(self.end_2, self.end_1, ax, col, style)\n",
        "        self.draw_line(self.end_3, self.end_2, ax, col, style)\n",
        "        return None\n",
        "\n",
        "    def rotate(self, rot_ax, vec, alpha): # вспомогательная функция для поворота звеньев\n",
        "        base1 = rot_ax\n",
        "        base2 = np.cross(rot_ax, vec)\n",
        "        base3 = np.cross(base1, base2) # базис относительных координат на основе оси повората и поворачиваемого вектора\n",
        "\n",
        "        base1 = base1 / np.linalg.norm(base1)\n",
        "        base2 = base2 / np.linalg.norm(base2)\n",
        "        base3 = base3 / np.linalg.norm(base3) # нормировка базисных векторов\n",
        "\n",
        "        base_mat = np.transpose(np.matrix([base1, base2, base3])) # матрица обратной замены базиса\n",
        "        base_inv = np.linalg.inv(base_mat) # матрица замены базиса\n",
        "\n",
        "        rot_mat = np.matrix([[1, 0, 0], [0, np.cos(alpha), -np.sin(alpha)], [0, np.sin(alpha), np.cos(alpha)]]) # матрица поворота на угол альфа вокург первого базисного вектора\n",
        "\n",
        "        new_vec = base_mat * (rot_mat * (base_inv * np.transpose(np.matrix(vec - rot_ax)))) # последовательно смена базиса, поворот и обратная смена базиса\n",
        "\n",
        "        rot_vec = np.array(np.transpose(new_vec))[0] + rot_ax\n",
        "\n",
        "        return rot_vec\n",
        "\n",
        "    def rotate_first_el(self, alpha):\n",
        "        if self.rot_1 + alpha > np. pi:\n",
        "            alpha = np.pi - self.rot_1\n",
        "        elif self.rot_1 + alpha < - np.pi:\n",
        "            alpha = - np.pi - self.rot_1\n",
        "\n",
        "        base_vec = np.array([0, 0, 1])\n",
        "        rot_end_1 = self.rotate(base_vec, self.end_1, alpha)\n",
        "        rot_end_2 = self.rotate(base_vec, self.end_2, alpha)\n",
        "        rot_end_3 = self.rotate(base_vec, self.end_3, alpha)\n",
        "\n",
        "        self.end_1 = rot_end_1\n",
        "        self.end_2 = rot_end_2\n",
        "        self.end_3 = rot_end_3\n",
        "        self.rot_1 += alpha\n",
        "\n",
        "    def rotate_second_el(self, alpha):\n",
        "        if self.rot_2 + alpha > np. pi:\n",
        "            alpha = np.pi - self.rot_2\n",
        "        elif self.rot_2 + alpha < - np.pi:\n",
        "            alpha = - np.pi - self.rot_2\n",
        "\n",
        "        rot_end_2 = self.rotate(self.end_1, self.end_2, alpha)\n",
        "        rot_end_3 = self.rotate(self.end_1, self.end_3, alpha)\n",
        "\n",
        "        self.end_2 = rot_end_2\n",
        "        self.end_3 = rot_end_3\n",
        "        self.rot_2 += alpha\n",
        "        return None\n",
        "\n",
        "    def rotate_third_el(self, alpha):\n",
        "        if self.rot_3 + alpha > np. pi:\n",
        "            alpha = np.pi - self.rot_3\n",
        "        elif self.rot_3 + alpha < - np.pi:\n",
        "            alpha = - np.pi - self.rot_3\n",
        "\n",
        "        rot_end_3 = self.rotate(self.end_2, self.end_3, alpha)\n",
        "\n",
        "        self.end_3 = rot_end_3\n",
        "        self.rot_3 += alpha\n",
        "        return None\n",
        "\n",
        "    def slope_first_el(self, alpha):\n",
        "        if self.slope_1 + alpha > np. pi:\n",
        "            alpha = np.pi - self.slope_1\n",
        "        elif self.slope_1 + alpha < - np.pi:\n",
        "            alpha = - np.pi - self.slope_1\n",
        "\n",
        "        base1 = np.array([self.end_1[0], self.end_1[1], 0])\n",
        "        base2 = np.array([0, 0, 1])\n",
        "        base3 = np.cross(base1, base2)\n",
        "\n",
        "\n",
        "        base1 = base1 / np.linalg.norm(base1)\n",
        "        base2 = base2 / np.linalg.norm(base2)\n",
        "        base3 = base3 / np.linalg.norm(base3) # нормировка базисных векторов\n",
        "\n",
        "        base_mat = np.transpose(np.matrix([base3, base2, base1])) # матрица обратной замены базиса\n",
        "        base_inv = np.linalg.inv(base_mat) # матрица замены базиса\n",
        "\n",
        "        rot_mat = np.matrix([[1, 0, 0], [0, np.cos(alpha), -np.sin(alpha)], [0, np.sin(alpha), np.cos(alpha)]]) # матрица поворота на угол альфа вокург первого базисного вектора\n",
        "\n",
        "        new_end_1 = base_mat * (rot_mat * (base_inv * (np.transpose(np.matrix(self.end_1)))))\n",
        "        new_end_1 = np.array(np.transpose(new_end_1))[0]\n",
        "        new_end_2 = base_mat * (rot_mat * (base_inv * (np.transpose(np.matrix(self.end_2)))))\n",
        "        new_end_2 = np.array(np.transpose(new_end_2))[0]\n",
        "        new_end_3 = base_mat * (rot_mat * (base_inv * (np.transpose(np.matrix(self.end_3)))))\n",
        "        new_end_3 = np.array(np.transpose(new_end_3))[0]\n",
        "\n",
        "        self.end_1 = new_end_1\n",
        "        self.end_2 = new_end_2\n",
        "        self.end_3 = new_end_3\n",
        "        self.slope_1 += alpha\n",
        "\n",
        "        return None\n",
        "\n",
        "    def slope_second_el(self, alpha):\n",
        "        if self.slope_2 + alpha > np. pi:\n",
        "            alpha = np.pi - self.slope_2\n",
        "        elif self.slope_2 + alpha < - np.pi:\n",
        "            alpha = - np.pi - self.slope_2\n",
        "\n",
        "        base1 = np.cross(self.end_1, self.end_2)\n",
        "        base2 = self.end_1\n",
        "        base3 = np.cross(base1, base2)\n",
        "\n",
        "        base1 = base1 / np.linalg.norm(base1)\n",
        "        base2 = base2 / np.linalg.norm(base2)\n",
        "        base3 = base3 / np.linalg.norm(base3) # нормировка базисных векторов\n",
        "\n",
        "        base_mat = np.transpose(np.matrix([base1, base2, base3])) # матрица обратной замены базиса\n",
        "        base_inv = np.linalg.inv(base_mat) # матрица замены базиса\n",
        "\n",
        "        rot_mat = np.matrix([[1, 0, 0], [0, np.cos(alpha), -np.sin(alpha)], [0, np.sin(alpha), np.cos(alpha)]]) # матрица поворота на угол альфа вокург первого базисного вектора\n",
        "\n",
        "        new_end_2 = base_mat * (rot_mat * (base_inv * (np.transpose(np.matrix(self.end_2 - self.end_1)))))\n",
        "        new_end_2 = np.array(np.transpose(new_end_2))[0] + self.end_1\n",
        "\n",
        "        new_end_3 = base_mat * (rot_mat * (base_inv * (np.transpose(np.matrix(self.end_3 - self.end_1)))))\n",
        "\n",
        "        new_end_3 = np.array(np.transpose(new_end_3))[0] + self.end_1\n",
        "\n",
        "        self.end_2 = new_end_2\n",
        "        self.end_3 = new_end_3\n",
        "        self.slope_2 += alpha\n",
        "\n",
        "        return None\n",
        "\n",
        "    def slope_third_el(self, alpha):\n",
        "        base1 = np.cross(self.end_2 - self.end_1, self.end_3 - self.end_1)\n",
        "        base2 = self.end_2 - self.end_1\n",
        "        base3 = np.cross(base1, base2)\n",
        "\n",
        "        base1 = base1 / np.linalg.norm(base1)\n",
        "        base2 = base2 / np.linalg.norm(base2)\n",
        "        base3 = base3 / np.linalg.norm(base3) # нормировка базисных векторов\n",
        "\n",
        "        base_mat = np.transpose(np.matrix([base1, base2, base3])) # матрица обратной замены базиса\n",
        "        base_inv = np.linalg.inv(base_mat) # матрица замены базиса\n",
        "\n",
        "        rot_mat = np.matrix([[1, 0, 0], [0, np.cos(alpha), -np.sin(alpha)], [0, np.sin(alpha), np.cos(alpha)]]) # матрица поворота на угол альфа вокург первого базисного вектора\n",
        "\n",
        "        new_end_3 = base_mat * (rot_mat * (base_inv * (np.transpose(np.matrix(self.end_3 - self.end_2)))))\n",
        "        new_end_3 = np.array(np.transpose(new_end_3))[0] + self.end_2\n",
        "        self.slope_3 += alpha\n",
        "\n",
        "        self.end_3 = new_end_3\n",
        "\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "6GuCMNXiRAH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Среда для агента"
      ],
      "metadata": {
        "id": "BrdmAU8EREeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Environment():\n",
        "      def __init__(self):\n",
        "            self.robot = None\n",
        "            self.start_pos = None\n",
        "            self.target = None\n",
        "            self.state = np.zeros(12)\n",
        "            self.max_len = 0\n",
        "            return None\n",
        "\n",
        "      def robot_On(self, a, b, c):\n",
        "          self.robot = Robot(a, b, c)\n",
        "          self.start_pos = [a, b, c]\n",
        "          self.state[0:3] = a\n",
        "          self.state[3:6] = b\n",
        "          self.state[6:9] = c\n",
        "          self.max_len = np.linalg.norm(a) + np.linalg.norm(b) + np.linalg.norm(c)\n",
        "          return None\n",
        "\n",
        "      def set_target(self):\n",
        "          max_len = np.linalg.norm(self.robot.end_1) + np.linalg.norm(self.robot.end_2) + np.linalg.norm(self.robot.end_3)\n",
        "          x = np.random.rand() * max_len\n",
        "          max_len -= np.linalg.norm(x)\n",
        "          y = np.random.rand() * max_len\n",
        "          max_len -= np.linalg.norm(y)\n",
        "          z = np.random.rand() * max_len\n",
        "\n",
        "          self.target = np.array([x, y, z])\n",
        "          self.state[9:12] = self.target\n",
        "          return None\n",
        "\n",
        "      def reset(self):\n",
        "          self.robot_On(self.start_pos[0], self.start_pos[1], self.start_pos[2])\n",
        "          self.set_target()\n",
        "          return self.state\n",
        "\n",
        "      def step(self, action):\n",
        "          prev_dist = np.linalg.norm(self.state[9:12] - self.state[6:9])\n",
        "\n",
        "          self.robot.slope_first_el(action[0])\n",
        "          self.robot.slope_second_el(action[1])\n",
        "          self.robot.slope_third_el(action[2])\n",
        "\n",
        "          self.robot.rotate_first_el(action[3])\n",
        "          self.robot.rotate_second_el(action[4])\n",
        "          self.robot.rotate_third_el(action[5])\n",
        "\n",
        "          self.state[0:3] = self.robot.end_1\n",
        "          self.state[3:6] = self.robot.end_2\n",
        "          self.state[6:9] = self.robot.end_3\n",
        "\n",
        "          dist = np.linalg.norm(self.state[9:12] - self.state[6:9])\n",
        "\n",
        "          if dist < 0.1:\n",
        "              reward = 100\n",
        "              done = True\n",
        "              return self.state, reward, done\n",
        "\n",
        "          reward = (prev_dist - dist) / self.max_len\n",
        "          done = False\n",
        "          return self.state, reward, done\n"
      ],
      "metadata": {
        "id": "u2tGcwtVRIpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Обучение"
      ],
      "metadata": {
        "id": "DPcgDzy_RMik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Параметры агента и среды\n",
        "states_dim = 12\n",
        "actions_dim = 6\n",
        "max_action = np.ones(6) * np.pi # обязан быть размерности actions_dim\n",
        "\n",
        "# Параметры симуляций\n",
        "episode_n = 2000\n",
        "session_n_mas = [200, 500] # количество сессий в эпизоде\n",
        "session_len_mas = [100] # количество пар состояние-дейтсвие за сессию\n",
        "q_param_mas = [0.9]\n",
        "grad_step = 0.01 # шаг при градиентном спуске\n",
        "\n",
        "# Параметры манипулятора\n",
        "L1, L2, L3 = 20, 15, 10\n",
        "\n",
        "x_1, y_1 = 2, 2.5\n",
        "z_1 = np.sqrt(L1 **2 - x_1 ** 2 - y_1 **2)\n",
        "end_1 = np.array([x_1, y_1, z_1])\n",
        "\n",
        "x_2, y_2 = 3, 4\n",
        "z_2 = np.sqrt(L2 **2 - x_2 ** 2 - y_2 **2)\n",
        "end_2 = np.array([x_2, y_2, z_2])\n",
        "\n",
        "x_3, y_3 = 4, 5.5\n",
        "z_3 = np.sqrt(L3 **2 - x_3 ** 2 - y_3 **2)\n",
        "end_3 = np.array([x_3, y_3, z_3])\n",
        "\n",
        "for session_n in session_n_mas:\n",
        "    for session_len in session_len_mas:\n",
        "        for q_param in q_param_mas:\n",
        "            env = Environment()\n",
        "            env.robot_On(end_1, end_2, end_3)\n",
        "            env.set_target()\n",
        "            agent = CrossEnthropyNNAgent(states_dim, actions_dim, max_action, grad_step=grad_step)\n",
        "            mean_total_reward = []\n",
        "            for episode in range(episode_n):\n",
        "                # Оценка политики (noise = 0)\n",
        "                sessions = [get_session(env, agent, session_len) for _ in range(session_n)]\n",
        "\n",
        "                mean_total_reward.append(np.mean([session['total_reward'] for session in sessions]))\n",
        "                # print('mean_total_reward = ', mean_total_reward[-1])\n",
        "\n",
        "                elite_sessions = get_elite_sessions(sessions, q_param)\n",
        "\n",
        "                # Корректировка политики\n",
        "                if len(elite_sessions) > 0:\n",
        "                    agent.update_policy(elite_sessions)\n",
        "\n",
        "            name = str(session_n) + '_' + str(session_len) + '_' + str(q_param) + '.txt'\n",
        "            dir = '_'############################################ ДИРЕКТОРИЯ 'D:\\py_ws\\'\n",
        "            with open(dir + name, 'w') as file:\n",
        "                file.write('session_n = ' + str(session_n) + '\\n')\n",
        "                file.write('session_len = ' + str(session_len) + '\\n')\n",
        "                file.write('q_param = ' + str(q_param) + '\\n')\n",
        "\n",
        "                for value in mean_total_reward:\n",
        "                    file.write(str(value) + '\\n')\n",
        "\n",
        "                k = 1\n",
        "                layers = [agent.network[0], agent.network[2], agent.network[4], agent.network[6]]\n",
        "                for layer in layers:\n",
        "                    weight = layer.weight.data.clone().detach().numpy()\n",
        "                    bias = layer.bias.data.clone().detach().numpy()\n",
        "\n",
        "                    file.write('Layer_num = ' + str(k) + ', weight' + '\\n')\n",
        "                    for w in weight:\n",
        "                        file.write(str(float(w)) + '\\n')\n",
        "\n",
        "                    file.write('Layer_num = ' + str(k) + ', bias' + '\\n')\n",
        "                    for b in bias:\n",
        "                        file.write(str(float(b)) + '\\n')\n",
        "\n",
        "                    k+=1\n"
      ],
      "metadata": {
        "id": "uDjrorFCjMne",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "ffdb4fb0-5bc3-4a28-aaa7-018f69ab1753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'grad_step' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-bb070bd21fb7>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobot_On\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEnthropyNNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mmean_total_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'grad_step' is not defined"
          ]
        }
      ]
    }
  ]
}